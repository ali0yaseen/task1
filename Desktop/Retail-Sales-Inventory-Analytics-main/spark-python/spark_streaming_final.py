import os
import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, when, udf, lit, current_timestamp
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© Ù„Ø¶Ù…Ø§Ù† Ø§Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ ÙˆÙŠÙ†Ø¯ÙˆØ²
os.environ['PYSPARK_PYTHON'] = sys.executable
os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

# 2. Ø¥Ø¹Ø¯Ø§Ø¯ Ø¬Ù„Ø³Ø© Ø§Ù„Ø³Ø¨Ø§Ø±Ùƒ (Ù…Ø¹ ØªØµØ­ÙŠØ­ Ø§Ù„Ø¥ØµØ¯Ø§Ø±Ø§Øª Ù„Ù€ Spark 3.5)
spark = SparkSession.builder \
    .appName("RetailInventoryAnalyticsFinal") \
    .config("spark.mongodb.output.uri", "mongodb://127.0.0.1/inventory_db.alerts") \
    .config("spark.jars.packages", "org.mongodb.spark:mongo-spark-connector_2.12:3.0.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# --- 3. Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ù†Ø·Ù‚ Ø§Ù„Ù€ Bloom Filter (Ø¹Ø±ÙˆØ¶ Ø§Ù„Ø®ØµÙ…) ---
# Ù…Ø­Ø§ÙƒØ§Ø© Ù„Ù„ÙÙ„ØªØ±: Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ù…Ù† 1 Ø¥Ù„Ù‰ 50 Ù‡ÙŠ Ø§Ù„Ù…Ø´Ù…ÙˆÙ„Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¶
flash_sale_ids = [str(i) for i in range(1, 51)]

@udf(returnType=StringType())
def check_promo_eligibility(item_id):
    if item_id in flash_sale_ids:
        return "ğŸ”¥ FLASH SALE - 20% OFF"
    return "Regular Price"

# --- 4. ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù€ Schema (Ù…Ø·Ø§Ø¨Ù‚ Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù€ Producer) ---
schema = StructType([
    StructField("item_id", StringType(), True),
    StructField("item_name", StringType(), True),
    StructField("event_type", StringType(), True),
    StructField("reported_stock", IntegerType(), True)
])

# --- 5. Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Kafka ---
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "first-topic") \
    .option("startingOffsets", "latest") \
    .load()

# --- 6. Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ùˆ Ø§Ù„Ù€ Watermarking ---
processed_df = df.selectExpr("CAST(value AS STRING)") \
    .select(from_json(col("value"), schema).alias("data")) \
    .select("data.*") \
    .withColumn("timestamp", current_timestamp()) \
    .withWatermark("timestamp", "10 minutes")

# --- 7. ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù„ÙˆØ¬ÙŠÙƒ Ø§Ù„ØªØ­Ù„ÙŠÙ„ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ (Ù†ÙØ³ Ù…Ù†Ø·Ù‚ Ø§Ù„Ø³ÙƒØ§Ù„Ø§) ---
REORDER_POINT = 50

clean_final_df = processed_df \
    .withColumn("Offer", check_promo_eligibility(col("item_id"))) \
    .withColumn("Inventory_State",
                when(col("reported_stock") < REORDER_POINT, "âš ï¸ Low Stock - REORDER NOW")
                .when(col("reported_stock") > 1000, "ğŸ“¢ OVERSTOCK")
                .otherwise("Normal")) \
    .withColumn("Behavior_Analysis",
                when((col("event_type") == "SALE") & (col("reported_stock") < 10), "ğŸš¨ Critical Anomalies")
                .otherwise("Normal")) \
    .withColumn("Suggested_Order_Qty",
                when(col("reported_stock") < REORDER_POINT, lit(100))
                .otherwise(lit(0))) \
    .select(
    col("item_id").alias("_id"),             # Ø¬Ø¹Ù„ Ø±Ù‚Ù… Ø§Ù„Ù…Ù†ØªØ¬ Ù‡Ùˆ Ø§Ù„Ù…Ø¹Ø±Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ ÙÙŠ MongoDB
    col("item_name").alias("Product"),
    col("event_type").alias("Type"),
    col("reported_stock").alias("Stock"),
    col("Offer"),
    col("Inventory_State"),
    col("Behavior_Analysis"),
    col("Suggested_Order_Qty")
)

# --- 8. Ø¯Ø§Ù„Ø© Ø§Ù„Ø­ÙØ¸ Ù„Ù€ MongoDB ---
def write_to_mongo(batch_df, batch_id):
    batch_df.write \
        .format("com.mongodb.spark.sql.DefaultSource") \
        .option("database", "inventory_db") \
        .option("collection", "alerts") \
        .mode("append") \
        .save()

# --- 9. ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨Ø« (Ø§Ù„ÙƒÙˆÙ†Ø³ÙˆÙ„ ÙˆØ§Ù„Ù…ÙˆÙ†Ø¬Ùˆ) ---
# Ù…Ù„Ø§Ø­Ø¸Ø©: Ø§Ø³ØªØ®Ø¯Ù…Ù†Ø§ checkpointLocation Ù…Ø®ØªÙ„Ù Ù„ÙƒÙ„ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… Ø§Ù„ØªØ¯Ø§Ø®Ù„
query_mongo = clean_final_df.writeStream \
    .foreachBatch(write_to_mongo) \
    .option("checkpointLocation", "C:/bigdata/checkpoints/py_mongo_vfinal") \
    .start()

query_console = clean_final_df.writeStream \
    .outputMode("append") \
    .format("console") \
    .option("truncate", "false") \
    .option("checkpointLocation", "C:/bigdata/checkpoints/py_console_vfinal") \
    .start()

# Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø­ØªÙ‰ Ø§Ù†ØªÙ‡Ø§Ø¡ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª
spark.streams.awaitAnyTermination()